\chapter{Calcolo del Google Pagerank}
\label{chap:Google}

\subsection{Esercizio 1}
\label{sub:Es1}
[Teorema di Gershgorin]
      Dimostrare che gli autovalori di una matrice
      $A=(a_{ij})\in\mathbb{C}^{n n}$
      \emph{sono contenuti nell'insieme}
			\[
				\mathcal{D}=\bigcup_{i=1}^n\mathcal{D}_i,\qquad \mathcal{D}_i=\left\{\lambda\in\mathbb{C}:|\lambda-a_{ii}|\leq\sum_{\begin{subarray}{c}
					j=1\\
					j\neq i
				\end{subarray}}^n|a_{ij}|\right\},\quad i=1,\dots,n.
			\]
\begin{sol}
   Sia $\lambda\in\sigma{A}$ ed $\underline{x}$ il corrispondente autovettore ($\underline{x}\neq\underline{0}$
   e $A\underline{x}=\lambda\underline{x}$) quindi $\underline{e}_i^TA\underline{x}=\lambda\underline{e}_i^T\underline{x}$
   per $i=1,\ldots,n$ cioè $\sum_{j=1}^n{a_{i,j}x_j}=\lambda x_i$;\\
   posto $i$ tale che $|x_i|\geq|x_j|$ ($x_i\neq 0$)
   risulta $$\lambda=\frac{\sum_{j=1}^n{a_{i,j}x_j}}{x_i}=a_{i,i}+\sum_{j=1\: j\neq i}^n{a_{i,j}\frac{x_j}{x_i}}$$ ovvero
   $\lambda-a_{i,i}=\sum_{j=1\: j\neq i}^n{a_{i,j}\frac{x_j}{x_i}}$.\\
   Mettendo poi i valori assuluti
   $$\left|\lambda-a_{i,i}\right|=\left|\sum_{j=1\: j\neq i}^n{a_{i,j}\frac{x_j}{x_i}}\right|\leq\sum_{j=1\: j\neq i}^n{\left|a_{i,j}\frac{x_j}{x_i}\right|}\leq\sum_{j=1\: j\neq i}^n{\left|a_{i,j}\right|}$$ poiché $\left|\frac{x_j}{x_i}\right|\leq 1$
   in quanto $|x_i|\geq|x_j|$. Segue $\lambda\in\bigcup_{i=1}^{n}{\mathcal{D}_i}$ da cui $\sigma{A}\subseteq\mathcal{D}$.
\end{sol}
\subsection{Esercizio 2}
\label{sub:Es2}
\emph{
      Utilizzare il metodo delle potenze per approssimare l'autovalore dominante della matrice
			\[
				A_n=\begin{pmatrix}
					2 & -1 & &\\
					-1 & 2 & \ddots &\\
					& \ddots & \ddots & -1\\
					& & -1 & 2
				\end{pmatrix}\in\mathbb{R}^{n\times n},
			\]
			per valori crescenti di $n$. Verificare numericamente che questo è dato da $2\left(1+\cos\frac{\pi}{n+1}\right)$.
}
\begin{sol}
  \normalfont
  $A_n$ è una M-matrice,in quanto può essere scritta come : $A_n=2(I_n-B_n)$ dove
  $$B_n=\begin{pmatrix}0&1/2&&\\1/2&0&\ddots&\\&\ddots&\ddots&1/2\\&&1/2&0\end{pmatrix}\in\mathbb{R}^{n\times n} \quad I_n=\begin{pmatrix}2&0&&\\0&2&\ddots&\\&\ddots&\ddots&0\\&&0&2\end{pmatrix}\in\mathbb{R}^{n\times n}.$$
  Risulta $\lambda_j(B_n)=\cos{\frac{j\pi}{n+1}}$, $|\lambda_j|\leq 1$, per $j=1,\ldots,n$;
  segue $\lambda_j(A_n)=2(\lambda_j(I_n)-\lambda_j(B_n))=2(1-\cos{\frac{j\pi}{n+1}})$; il massimo è $\lambda=2(1+\cos{\frac{\pi}{n+1}})$.
  La verifica numerica di questo risultato è riportata nelle seguenti tabelle.
  \begin{center}\begin{tabular}{c|c|c|c}
    \hline\multicolumn{4}{c}{Tolleranza $tol=10^{-5}$}\\\hline
    $n$ & $\lambda_1$ & $2\left(1+\cos{\frac{\pi}{n+1}}\right)$ & scostamento\\\hline
    10 & 3.9189 & 3.9190 & 0.0001 \\
    15 & 3.9614 & 3.9616 & 0.0002 \\
    20 & 3.9774 & 3.9777 & 0.0003 \\
    25 & 3.9853 & 3.9854 & 0.0002 \\
    30 & 3.9891 & 3.9897 & 0.0006 \\
    35 & 3.9921 & 3.9924 & 0.0003 \\
    40 & 3.9929 & 3.9941 & 0.0012 \\
    45 & 3.9938 & 3.9953 & 0.0015 \\
    50 & 3.9947 & 3.9962 & 0.0015
  \end{tabular}\end{center}

  \begin{center}\begin{tabular}{c|c|c|c}
    \hline\multicolumn{4}{c}{Tolleranza $tol=10^{-7}$}\\\hline
    $n$ & $\lambda_1$ & $2\left(1+\cos{\frac{\pi}{n+1}}\right)$ & scostamento\\\hline
    5 & 3.7321 & 3.7321 & 0.0000 \\
    10 & 3.9190 & 3.9190 & 0.0000 \\
    15 & 3.9616 & 3.9616 & 0.0000 \\
    20 & 3.9777 & 3.9777 & 0.0000 \\
    25 & 3.9854 & 3.9854 & 0.0000 \\
    30 & 3.9897 & 3.9897 & 0.0000 \\
    35 & 3.9924 & 3.9924 & 0.0000 \\
    40 & 3.9941 & 3.9941 & 0.0000 \\
    45 & 3.9953 & 3.9953 & 0.0000 \\
    50 & 3.9962 & 3.9962 & 0.0000
  \end{tabular}\end{center}
\end{sol}

\subsection{Esercizio 3}
\label{sub:es3}
\emph{Dimostrare i Corollari 6.2 e 6.3.}
\begin{sol}
  \normalfont
  \underline{Corollario 6.2}\\
  Se $A=\alpha(I-B)$ è una M-matrice e $A=M-N$, con $0\leq N\leq\alpha B$, allora M è nonsingolare
  e lo splitting è regolare. Pertanto, il metodo iterativo per calcolare un’approssimazione
  del vettore di pagerank è convergente.
  \\
  \underline{Dimostrazione}\\
  Sia $A=\alpha(I-B)=M-N$ con $0\leq N\leq\alpha B$ quindi $M=\alpha I-\alpha B+N=\alpha I-\alpha B+\alpha Q=\alpha(I-(B-Q))$
  con $\alpha Q=N\leq\alpha B$ e $0\leq Q\leq B$. Dato che $0\leq B-Q\leq B$, per il Lemma 6.2,
  $\rho(B-Q)\leq\rho(B)\leq 1$. Quindi $M$ è una M-matrice; lo splitting è regolare infatti
  $M^{-1}\geq 0$ ($M$ è una M-matrice) e $B-Q\geq 0$.\\
  \vspace{5mm}
  \underline{Corollario 6.3}\\
  Se $A$ è una M-matrice e la matrice $M$ in ($A=M-N$) è ottenuta ponendo a $0$
  gli elementi extradiagonali di $A$, allora lo splitting ($A=M-N$) è regolare.
  Pertanto il metodo iterativo è convergente.\\
  \underline{Dimostrazione}\\
  Sia $A=M-N$ M-matrice con $M=diag(A)$ allora la matrice $A-M=B$ avrà elementi nulli sulla diagonale e, altrove,
  gli elementi di $A$. Poiché $A=\alpha(I-B)=\alpha I-\alpha B=M-N$, risulta $\alpha B=N$ quindi,
  per il Corollario 6.2, lo splitting è regolare ed il metodo è convergente.
\end{sol}

\subsection{Esercizio 4}
\label{sub:es4}
\emph{Dimostrare il Teorema 6.9.}
\begin{sol}
  \underline{Teorema 6.9}\\
  Se la matrice $A$ in ($A=D-L-U$) è una M-matrice, allora $D,L,U\geq 0$.
   In particolare $D$ ha elementi diagonali positivi ($D>0$).\\
  \underline{Dimistrazione}\\
  Poiché $A$ è una M-matrice, essa può essere scritta nella forma $A=\alpha(I-B)$ con $B\geq 0$ e $\rho(B)<1$;
   inoltre, per ipotesi, $A=D-L-U$ da cui si deduce che $D=\alpha(I-diag(B))$ e $(L+U)=\alpha(B-diag(B))$.\\
  La matrice $(L+U)=\alpha(B-diag(B))$ risulta maggiore di zero in quanto $B>0\rightarrow (B-diag(B))>0$.
  La matrice $D$ ha elementi positivi sulla diagonale: supponiamo per assurdo $a_{i,i}\leq 0$:
    l'$i$-esima riga di $A$, negativa, è data da $A\underline{e}_i\leq\underline{0}$ dato che $A$ è una M-matrice risulta
    $\underline{e}_i\leq A^{-1}\underline{0}=\underline{0}$ assurdo poiché $\underline{e}_i\geq\underline{0}, \forall i$.
\end{sol}
\subsection{Esercizio 5}
\label{sub:es5}
\emph{Tenendo conto della (6.10), riformulare il metodo delle potenze (6.11) per il calcolo del \textit{Google pagerank} come metodo iterativo definito da uno splitting regolare.}
\begin{sol}
  Il problema del \textsl{Google pagerank} è $S(p)\hat{\underline{x}}=\hat{\underline{x}}$ dove $S(p)=pS+(1-p)\underline{v}\:\underline{e}^{T}$. Sostituendo, risulta
   $$(pS+(1-p)\underline{v}\:\underline{e}^{T})\hat{\underline{x}}=\hat{\underline{x}}\quad\Rightarrow\quad (I-pS)\hat{\underline{x}}=(1-p)\underline{v}\:\underline{e}^T\hat{\underline{x}}$$
  Dato che $\underline{v}=\frac{1}{n}\underline{e}$ ed $e^T\hat{\underline{x}}=1$ si ricava
  $$(I-pS)\hat{\underline{x}}=\frac{1-p}{n}\underline{e}.$$
  Si può infine definire il seguente metodo iterativo:
  $$I\underline{x}_{k+1}=pS\underline{x}_k+\dfrac{1-p}{n}\underline{e}.$$
  Il metodo è convergente in quanto la matrice di iterazione ha raggio spettrale minore di $1$: $\rho(I^{-1}pS)=\rho(pS)<1$ dato che $p \in (0,1)$ e $\rho(S)=1$.
\end{sol}

\subsection{Esercizio 6}
\label{sub:es6}
\emph{Dimostrare che il metodo di Jacobi converge asintoticamente in un numero minore di iterazioni, rispetto al metodo delle potenze (6.11) per il calcolo del \textit{Google pagerank}.}
\begin{sol}
  La matrice di iterazione di Jacobi ha raggio spettrale minore di 1 mentre, nel calcolo del \textsl{Google pagerank},
   l'autovalore dominante è $\lambda=1$ quindi il raggio spettrale di tale matrice è esattamente 1.
   Poiché il raggio spettrale della matrice di iterazione del metodo di Jacobi è minore del raggio spettrale della matrice del
   \textsl{Google pagerank}, il metodo di Jacobi converge in asintoticamente in un numero minore di iterazioni, rispetto al metodo delle potenze per il calcolo del \textsl{Google pagerank}.
\end{sol}

\subsection{Esercizio 7}
\label{sub:es7}
\emph{Dimostrare che, se $A$ è diagonale dominante, per riga o per colonna, il metodo di Jacobi è convergente.}
\begin{sol}
  Nel metodo di Jacobi si ha $A=M-N=D-(L+U)$ quindi
   $$M=\begin{pmatrix}a_{1,1}&&&\\&\ddots&&\\&&\ddots&\\&&&a_{n,n}\end{pmatrix}\mbox{ e } N=\begin{pmatrix}0&a_{1,2}&\ldots&a_{1,n}\\a_{2,1}&\ddots&\ddots&\vdots\\\vdots&\ddots&\ddots&a_{n-1,n}\\a_{n,1}&\ldots&a_{n,n-1}&0\end{pmatrix}.$$
   Si ha $$B=M^{-1}N=\begin{pmatrix}\frac{1}{a_{1,1}}&&&\\&\ddots&&\\&&\ddots&\\&&&\frac{1}{a_{n,n}}\end{pmatrix}\begin{pmatrix}0&a_{1,2}&\ldots&a_{1,n}\\a_{2,1}&\ddots&\ddots&\vdots\\\vdots&\ddots&\ddots&a_{n-1,n}\\
   a_{n,1}&\ldots&a_{n,n-1}&0\end{pmatrix}=$$$$=\begin{pmatrix}0&\frac{a_{1,2}}{a_{1,1}}&\ldots&\frac{a_{1,n}}{a_{1,1}}\\\frac{a_{2,1}}{a_{2,2}}&\ddots&\ddots&\vdots\\\vdots&\ddots&\ddots&\frac{a_{n-1,n}}{a_{n-1,n-1}}\\\frac{a_{n,1}}{a_{n,n}}&\ldots&\frac{a_{n,n-1}}{a_{n,n}}&0\end{pmatrix},$$
   per il Teorema di Gershgorin risulta
   $$\mathcal{D}_i=\left\{\lambda\in\mathbb{C}\::\:|\lambda-b_{i,i}|\leq\sum_{j=1\: j\neq i}^{n}{|b_{i,j}|}\right\}=\left\{\lambda\in\mathbb{C}\::\:|\lambda|\leq\sum_{j=1\: j\neq i}^{n}{\left|\frac{a_{i,j}}{a_{i,i}}\right|}\right\};$$
   supposta $A$ a diagonale dominante per righe, $|a_{i,i}|\geq\sum_{j=1,\: j\neq i}^n{|a_{i,j}|}$,
   risulta $|\lambda|\leq\frac{1}{|a_{i,i}|}\sum_{j=1\: j\neq i}^{n}{a_{i,j}}<1$. Ogni $\mathcal{D}_i$ è centrato in $0$ e ha raggio minore di $1$ quindi $\mathcal{D}=\bigcup_{i=1}^{n}{\mathcal{D}_i}$ è centrato in $0$ e ha raggio pari al raggio massimo dei $\mathcal{D}_i$ ma sempre minore di $1$. Dato che $\lambda(A)=\lambda(A^T)$,
   il risultato vale anche se $A$ è a diagonale dominante per colonne; il metodo di Jacobi è dunque convergente per matrici a diagonale dominante.
\end{sol}

\subsection{Esercizio 8}
\label{sub:es8}
\emph{Dimostrare che, se $A$ è diagonale dominante, per riga o per colonna, il metodo di Gauss-Seidel è convergente.}
\begin{sol}
  Nel metodo di Gauss-Seidel si ha $A=M-N=(D-L)-U$;
   sia $\lambda\in\sigma(M^-1N)$ quindi $\lambda$ è tale che
   $\det{(M^{-1}N-\lambda I)}=\det{(M^{-1}(N-\lambda M))}=\det{(M^{-1})}\det{(N-\lambda M)}=0$.
   Dato che, per definizione di splitting, $\det{(M^{-1})}\neq 0$ deve risultare
   $\det{(N-\lambda M)}=0=\det{(\lambda M-N)}$; sia $H=\lambda M-N$ matrice singolare e supponiamo, per assurdo,
   $|\lambda|\geq 1$. Risulta $$H=\begin{cases}\lambda a_{i,j}&\mbox{se }i\geq j,\\a_{i,j}&\mbox{altrimenti},\end{cases};$$
   quindi $H$ è a diagonale dominante ma $$\sum_{j=1\: j\neq i}^n{|h_{i,j}|}\leq |\lambda|\sum_{j=1\: j\neq i}^n{|a_{i,j}|}<|\lambda||a_{i,i}|=|h_{i,i}|.$$
   Si ha una contraddizione poiché le matrici a diagonale dominanti sono non singolari,
   dunque $|\lambda|<1$ e quindi il metodo di Gauss-Seidel è convergente per matrici a diagonale dominante.
\end{sol}

\subsection{Esercizio 9}
\label{sub:es9}
\emph{Se $A$ è \textit{sdp}, il metodo di Gauss-Seidel risulta essere convergente.
      Dimostrare questo risultato nel caso (assai più semplice) in cui l'autovalore di massimo modulo della matrice di iterazione sia reale.\\
			(\underline{Suggerimento:} considerare il sistema lineare equivalente
			$$(D^{-\frac{1}{2}}AD^{-\frac{1}{2}})(D^{\frac{1}{2}}\underline{x})=(D^{-\frac{1}{2}}\underline{b}),\qquad D^{\frac{1}{2}}=diag(\sqrt{a_{11}},\dots,\sqrt{a_{nn}}),$$
			la cui matrice dei coefficienti è ancora \textit{sdp} ma ha diagonale unitaria,
      ovvero del tipo $I-L-L^T$. Osservare quindi che, per ogni vettore reale $\underline{v}$ di norma $1$,si ha:
      $\underline{v}^TL\underline{v}=\underline{v}^TL^T\underline{v}=\frac{1}{2}\underline{v}^T(L+L^T)\underline{v}<\frac{1}{2}$.)}
\begin{sol}
  Scriviamo il sistema $A\underline{x}=\underline{b}$ nella forma equivalente
  $$\left(D^{-1/2}AD^{-1/2}\right)\left(D^{1/2}\underline{x}\right)=\left(D^{-1/2}\underline{b}\right)$$
  con $D=diag(\sqrt{a_{1,1}},\ldots,\sqrt{a_{n,n}})$.
  La matrice $C=\left(D^{-1/2}AD^{-1/2}\right)$ ha diagonale unitaria:
  $$c_{i,i}=d_i^{-1}a_{i,i}d_i^{-1}=\frac{1}{\sqrt{a_{i,i}}}a_{i,i}\frac{1}{\sqrt{a_{i,i}}}=a_{i,i};$$
  inoltre è ancora sdp e scrivibile come $C=I-L-L^T$.\\
  Poiché $C$ è sdp risulta $\underline{v}^TA\underline{v}>0, \forall\underline{v}\neq\underline{0}$
  cioè $$\underline{v}^T\underline{v}>\underline{v}^TL\underline{v}+\underline{v}^TL^T\underline{v}\quad\Rightarrow\quad\underline{v}^TL\underline{v}=\underline{v}^TL^T\underline{v}<\frac{1}{2}.$$
  Sia $|\lambda|=\rho(M_{GS}^{-1}N_{GS})=\rho\left((I-L)^{-1}L^T\right)$
  assunto reale e $\underline{v}$ il corrispondente autovettore, dunque
  $$(I-L)^{-1}L^T=\lambda\underline{v}\quad\Rightarrow\quad\lambda\underline{v}=L^T\underline{v}+\lambda L\underline{v}$$
  ovvero $\lambda=\underline{v}^TL\underline{v}+\lambda\underline{v}^TL\underline{v}=(1+\lambda)\underline{v}^TL)\underline{v}$
  da cui $$\frac{\lambda}{1+\lambda}=\underline{v}^TL\underline{v}<\frac{1}{2}\quad\Rightarrow\quad -1<\lambda<1.$$
  Segue $\rho(M_{GS}^{-1}N_{GS})=|\lambda|<1$.
\end{sol}

\subsection{Esercizio 10}
\label{sub:es10}
\emph{Con riferimento ai vettori errore (6.16) e residuo (6.17) dimostrare che, se
			\begin{equation}
				\label{criterioArrestoSplitting}
				||\underline{r_k}||\leq\varepsilon||\underline{b}||,
			\end{equation}
			allora
			$$||\underline{e_k}||\leq\varepsilon k(A)||\underline{\hat{x}}||,$$
			dove $k(A)$ denota, al solito, il numero di condizionamento della matrice $A$.
      Concludere che, per sistemi lineari malcondizionati, anche la risoluzione iterativa (al pari di quella diretta)
      risulta essere più problematica.}
\begin{sol}
  Posto $\underline{e}_k=\underline{x}_k-\underline{\hat{x}}$
  e $\underline{r}_k=A\underline{x}_k-\underline{b}$, risulta
  $$A\underline{e}_k=A(\underline{x}_k-\underline{\hat{x}})=A\underline{x}_k-A\underline{\hat{x}}=A\underline{x}_k-b=\underline{r}_k.$$
  Segue, passando alle norme,
  \begin{equation}\begin{split}||\underline{e}_k||=&\left|\left|A^{-1}\underline{r}_k\right|\right|\leq\left|\left|A^{-1}\right|\right|\cdot||\underline{r}_k||\leq\left|\left|A^{-1}\right|\right|\cdot\varepsilon||\underline{b}||\leq\\\leq&\left|\left|A^{-1}\right|\right|\cdot\left|\left|A\right|\right|\cdot||\underline{\hat{x}}||=\varepsilon\kappa(A)||\underline{\hat{x}}||,\end{split}\end{equation}
  ovvero $$\frac{||\underline{e}_k||}{||\underline{\hat{x}}||}\leq\varepsilon\kappa(A).$$
  La risoluzione iterativa, come la risoluzione diretta, di sistemi lineari è ben condizionata per
  $\kappa(A)\approx 1$ mentre risulta malcondizionata per $\kappa(A)\gg 1$.
\end{sol}

\subsection{Esercizio 11}
\label{sub:es11}
\emph{Calcolare il polinomio caratteristico della matrice
			\[
				\begin{pmatrix}
					0 & \dots & 0 & \alpha\\
					1 & \ddots & & 0\\
					& \ddots & \ddots & \vdots\\
					0 & & 1 & 0
				\end{pmatrix}\in\mathbb{R}^{n\times n}.
			\]}
\begin{sol}
	\normalfont Il polinomio caratteristico è dato del determinante della matrice $A-\lambda I$:
	\begin{equation}
    \begin{split}\det{(A-\lambda I)}=&\det{\begin{pmatrix}-\lambda&\ldots&0&\alpha\\1&\ddots&&0\\&\ddots&\ddots&\vdots\\0&&1&-\lambda\end{pmatrix}}=\\=&(-1)^n\lambda^n+(-1)^{n+1}\alpha=(-1)^n(\lambda^n-\alpha).\end{split}
  \end{equation}
  Le radici di tale polinomio sono $\lambda=\sqrt[n]{\alpha}$.
\end{sol}

\subsection{Esercizio 12}
\label{sub:es12}
\emph{Dimostrare che i metodi di Jacobi e Gauss-Seidel possono essere utilizzati per la risoluzione del sistema lineare (gli elementi non indicati sono da intendersi nulli)
			\[
				\begin{pmatrix}
					1 & & & -\frac{1}{2}\\
					-1 & 1 & &\\
					& \ddots & \ddots &\\
					& & -1 & 1
				\end{pmatrix}\underline{x}=\begin{pmatrix}
					\frac{1}{2}\\
					0\\
					\vdots\\
					0
				\end{pmatrix}\in\mathbb{R}^n,
			\]
			la cui soluzione è $\underline{x}=(1,\dots,1)^T\in\mathbb{R}^n$.
      Confrontare il numero di iterazioni richieste dai due metodi per soddisfare lo stesso criterio di arresto (6.19),
      per valori crescenti di $n$ e per tolleranze $\varepsilon$ decrescenti. Riportare i risultati ottenuti in una tabella $(n/\varepsilon)$.
      }
  \begin{sol}
        \normalfont
        La matrice è una M-matrice:$$A=\begin{pmatrix}1&&&-1/2\\-1&1&&\\&\ddots&\ddots&\\&&-1&1\end{pmatrix}=I-B\mbox{ con }B=\begin{pmatrix}0&&&1/2\\1&\ddots&&\\&\ddots&\ddots&\\&&1&0\end{pmatrix}>0.$$
        Si dimostra che $\rho(B)<1$ calcolando gli autovalori della matrice $B$:
        $$\det(B-\lambda I)=$$ $$\det\begin{pmatrix}-\lambda&&&1/2\\1&\ddots&&\\&\ddots&\ddots&\\&&1&-\lambda\end{pmatrix}=-\lambda\det\begin{pmatrix}-\lambda&&&\\1&\ddots&&\\&\ddots&\ddots&\\&&1&-\lambda\end{pmatrix}_{(n-1)\times (n-1)}+$$ $$+(-1)^{n+1}\frac{1}{2}\det\begin{pmatrix}1&-\lambda &&\\&\ddots&\ddots&\\&&\ddots&-\lambda\\&&&1\end{pmatrix}_{(n-1)\times (n-1)} = -\lambda(-\lambda)^{n-1}+(-1)^{n+1}\frac{1}{2}=$$$$=(-1)^{n-2}\lambda^n+(-1)^{n+1}\frac{1}{2}=(-1)^n\frac{1}{2}(2\lambda^n-1).$$
        Quindi $\det(B-\lambda I)=0$ se e solo se $2\lambda^n-1=0$ se e solo se $\lambda=2^{-1/n}$. Poiché
        $|\lambda|<1$, $\rho(B)<1$ quindi $A$ è una M-matrice ed è possibile risolvere il sistema lineare tramite i metodo iterativi
        di Jacobi e Gauss-Seidel in quando lo splitting è regolare ed $A$ converge.\\
        Implementando il criterio d'arresto $||\underline{r}_k||\leq\varepsilon||\underline{b}||$, si ha convergenza nel numero di iterazioni riportate nelle seguenti tabelle:
        \footnotesize
        \begin{center}\begin{tabular}{|c||c|c|c|c|c|c|c|c|c|c|}
        \hline\multicolumn{11}{c}{Iterazioni del metodo di Jacobi}\\\hline
        $n \backslash \varepsilon $ & $10^{-1}$& $10^{-2}$& $10^{-3}$& $10^{-14}$& $10^{-5}$& $10^{-6}$& $10^{-7}$& $10^{-8}$& $10^{-9}$& $10^{-10}$\\\hline
        5& 25 & 34 & 54 & 74 & 85 & 105 & 124 & 139 & 157 & 171 \\\hline
        10& 50 & 72 & 116 & 145 & 181 & 211 & 250 & 276 & 304 & 342 \\\hline
        15& 87 & 125 & 180 & 230 & 284 & 326 & 379 & 430 & 465 & 524 \\ \hline
        20& 95 & 175 & 239 & 298 & 370 & 433 & 512 & 573 & 628 & 702 \\\hline
        25& 128 & 217 & 299 & 375 & 468 & 549 & 643 & 720 & 800 & 885 \\ \hline
        30& 162 & 265 & 378 & 475 & 570 & 659 & 762 & 870 & 964 & 1066 \\ \hline
        35& 199 & 311 & 436 & 533 & 662 & 775 & 905 & 1014 & 1120 & 1249 \\ \hline
        40& 214 & 384 & 494 & 635 & 755 & 889 & 1037 & 1157 & 1299 & 1429 \\ \hline
        45& 252 & 423 & 556 & 703 & 853 & 1020 & 1165 & 1327 & 1448 & 1607 \\ \hline
        50& 299 & 458 & 622 & 787 & 963 & 1108 & 1290 & 1473 & 1630 & 1789 \\\hline
        \end{tabular}\end{center}
        \begin{center}\begin{tabular}{|c||c|c|c|c|c|c|c|c|c|c|}
        \hline\multicolumn{11}{c}{Iterazioni del metodo di Gauss-Seidel}\\\hline
        $n \backslash \varepsilon $ & $10^{-1}$& $10^{-2}$& $10^{-3}$& $10^{-14}$& $10^{-5}$& $10^{-6}$& $10^{-7}$& $10^{-8}$& $10^{-9}$& $10^{-10}$\\\hline
        5 & 3 & 7 & 10 & 13 & 17 & 20 & 22 & 26 & 30 & 33 \\\hline
        10 & 1 & 6 & 10 & 13 & 16 & 20 & 23 & 26 & 27 & 33 \\\hline
        15 & 4 & 6 & 9 & 12 & 17 & 19 & 21 & 27 & 27 & 33 \\ \hline
        20 & 2 & 5 & 10 & 12 & 15 & 20 & 23 & 27 & 28 & 33 \\ \hline
        25 & 4 & 4 & 10 & 12 & 16 & 20 & 24 & 23 & 28 & 33 \\ \hline
        30 & 1 & 7 & 7 & 13 & 17 & 19 & 23 & 27 & 29 & 33 \\ \hline
        35 & 2 & 7 & 10 & 11 & 17 & 19 & 23 & 26 & 30 & 32 \\ \hline
        40 & 1 & 7 & 9 & 12 & 17 & 20 & 18 & 27 & 30 & 25 \\ \hline
        45 & 1 & 3 & 9 & 13 & 15 & 20 & 23 & 27 & 30 & 32 \\ \hline
        50 & 2 & 3 & 10 & 7 & 15 & 19 & 23 & 27 & 27 & 31 \\ \hline
        \end{tabular}\end{center}
        \normalsize Si nota come il numero di iterazioni richieste dal metodo di Gauss-Seidel sia molto minore del numero richiesto dal metodo di Jacobi, per ogni valore della tolleranza.
      \end{sol}
